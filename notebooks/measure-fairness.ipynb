{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Measure fairness\n",
    "\n",
    "In this third notebook we're going to measure how fair our model is. We'll cover the following topics:\n",
    "\n",
    "* Loading a pretrained model\n",
    "* Loading the testing dataset\n",
    "* Using the FairlearnDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pretrained model\n",
    "First, we're going to load the model that we trained in part 2.\n",
    "It's saved using `joblib`, so we'll use that to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('../models/model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've loaded the model, we're loading up the test dataset for the model.\n",
    "This dataset contains samples that we haven't used during training. We can be sure that the model isn't going to remember these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the FairlearnDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the model, and testing dataset, we can start looking at the fairness of the model.\n",
    "You would expect that male and female customers are equally likeky to get a credit card from our bank. So let's check if that's the case.\n",
    "\n",
    "The FairlearnDashboard needs a couple of things:\n",
    "\n",
    "* A set of sensitive features to test for\n",
    "* A set of names for the sensitive features\n",
    "* The ground truth for the model from the testing set.\n",
    "* The predictions made for the model based on the features in the testing set.\n",
    "\n",
    "When you create a new instance of the FairlearnDashboard you'll be asked to select a sensitive attribute and a metric.\n",
    "Once these are selected, you'll get a dashboard that displays two things:\n",
    "\n",
    "* The performance disparity: The difference in model performance between demographic groups\n",
    "* The prediction disparity: The difference between demographic groups in the fraction of samples resulting in a positive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_test['default.payment.next.month']\n",
    "predicted_outcomes = model.predict(df_test.drop(['SEX','default.payment.next.month'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c401d3368b4e6fbec7ff89853a7ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x17d8a1002c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FairlearnDashboard(\n",
    "    sensitive_features=df_test['SEX'],\n",
    "    sensitive_feature_names=['SEX'],\n",
    "    y_true=ground_truth,\n",
    "    y_pred=predicted_outcomes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
